{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f234a1c8-b7dc-47fc-ac4d-cd77472e87c2",
   "metadata": {},
   "source": [
    "# NLP Modeling\n",
    "\n",
    "- [Begin Here](#Begin-Here)\n",
    "- [Remember to do this](#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0710c442-3d01-4362-813d-9a7dd106d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import personal modules\n",
    "import src.acquire as ac\n",
    "\n",
    "# import modules from libraries\n",
    "#from prepare import basic_clean, lemmatize\n",
    "from pprint import pprint\n",
    "\n",
    "#import datascience libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import vizualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn modules including classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting Classifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n",
    "\n",
    "# Sklearn testing, evaluating, and managing model\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.multioutput import MultiOutputClassifier as MOC \n",
    "from sklearn.pipeline import Pipeline as pipeline \n",
    "\n",
    "# more classifiers\n",
    "from xgboost import XGBClassifier  # XG Boost Classifier\n",
    "from lightgbm import LGBMClassifier # Light Gradient Boost Classifier\n",
    "\n",
    "\n",
    "import nltk #Natural Language Tool Kit\n",
    "import re   #Regular Expressions\n",
    "\n",
    "# NLP related modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696849d-0db3-4281-a487-0dae6ca2efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb468c-2a02-42b5-b712-0d5aa0734d3d",
   "metadata": {},
   "source": [
    "indian_news = ac.scrape_news()\n",
    "\n",
    "indian_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0f89d-aa1b-4b9a-8242-e9f811ff5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = indian_news['content'][0]\n",
    "words = words.lower().replace(',', '').replace('.', '')\n",
    "words = pd.Series(words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007075d-598e-4f23-afa1-95d463cfb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = (pd.DataFrame({'count': words.value_counts()})\n",
    "             .assign(frequency=lambda df: words_df['count'] / words_df['count'].sum())\n",
    "             .assign(augmented_frequency=lambda df: df['frequency'] / df['frequency'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d926c6a-9768-48f2-ac44-5d61a8d3ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.head()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff0551-23e3-4aa3-bdac-98cd899b54fb",
   "metadata": {},
   "source": [
    "## Begin Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca93a1-1a15-4ae0-a8c4-2d28b876f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text: str) -> list:\n",
    "    'Another simple text cleaning function'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0399bd25-f2c6-42e0-a52a-d6d0000d61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> list:\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    text = (text.encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split() # tokenization\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0367736d-6388-44cc-b6c5-a33dde289145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once I'm drunk, then don't blame me: Man to wo...</td>\n",
       "      <td>A woman claimed that a man kept forcing her to...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man says SpiceJet locked flyers in boarding ga...</td>\n",
       "      <td>A vlogger named Soumil Agarwal took to Instagr...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBI books Amrapali Group's MD Anil Sharma and ...</td>\n",
       "      <td>The CBI has booked Amrapali Group's MD Anil Sh...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-residents from 10 nations can soon use UPI...</td>\n",
       "      <td>National Payments Corporation of India (NPCI) ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want people to stay for 10 yrs: US firm's bo...</td>\n",
       "      <td>Mark Neilson, a 35-year-old senior partner at ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Once I'm drunk, then don't blame me: Man to wo...   \n",
       "1  Man says SpiceJet locked flyers in boarding ga...   \n",
       "2  CBI books Amrapali Group's MD Anil Sharma and ...   \n",
       "3  Non-residents from 10 nations can soon use UPI...   \n",
       "4  I want people to stay for 10 yrs: US firm's bo...   \n",
       "\n",
       "                                             content  category  \n",
       "0  A woman claimed that a man kept forcing her to...  business  \n",
       "1  A vlogger named Soumil Agarwal took to Instagr...  business  \n",
       "2  The CBI has booked Amrapali Group's MD Anil Sh...  business  \n",
       "3  National Payments Corporation of India (NPCI) ...  business  \n",
       "4  Mark Neilson, a 35-year-old senior partner at ...  business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_news = ac.scrape_news()\n",
    "indian_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5958b045-2890-4532-8f43-a4f7f5a59f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_news['clean_text'] = indian_news['content'].apply(clean).apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06921455-8113-4d20-bcdf-a3b327634e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once I'm drunk, then don't blame me: Man to wo...</td>\n",
       "      <td>A woman claimed that a man kept forcing her to...</td>\n",
       "      <td>business</td>\n",
       "      <td>woman claimed man kept forcing move aisle seat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man says SpiceJet locked flyers in boarding ga...</td>\n",
       "      <td>A vlogger named Soumil Agarwal took to Instagr...</td>\n",
       "      <td>business</td>\n",
       "      <td>vlogger named soumil agarwal took instagram cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBI books Amrapali Group's MD Anil Sharma and ...</td>\n",
       "      <td>The CBI has booked Amrapali Group's MD Anil Sh...</td>\n",
       "      <td>business</td>\n",
       "      <td>cbi booked amrapali group md anil sharma six o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-residents from 10 nations can soon use UPI...</td>\n",
       "      <td>National Payments Corporation of India (NPCI) ...</td>\n",
       "      <td>business</td>\n",
       "      <td>national payment corporation india npci permit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want people to stay for 10 yrs: US firm's bo...</td>\n",
       "      <td>Mark Neilson, a 35-year-old senior partner at ...</td>\n",
       "      <td>business</td>\n",
       "      <td>mark neilson 35yearold senior partner life ins...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Once I'm drunk, then don't blame me: Man to wo...   \n",
       "1  Man says SpiceJet locked flyers in boarding ga...   \n",
       "2  CBI books Amrapali Group's MD Anil Sharma and ...   \n",
       "3  Non-residents from 10 nations can soon use UPI...   \n",
       "4  I want people to stay for 10 yrs: US firm's bo...   \n",
       "\n",
       "                                             content  category  \\\n",
       "0  A woman claimed that a man kept forcing her to...  business   \n",
       "1  A vlogger named Soumil Agarwal took to Instagr...  business   \n",
       "2  The CBI has booked Amrapali Group's MD Anil Sh...  business   \n",
       "3  National Payments Corporation of India (NPCI) ...  business   \n",
       "4  Mark Neilson, a 35-year-old senior partner at ...  business   \n",
       "\n",
       "                                          clean_text  \n",
       "0  woman claimed man kept forcing move aisle seat...  \n",
       "1  vlogger named soumil agarwal took instagram cl...  \n",
       "2  cbi booked amrapali group md anil sharma six o...  \n",
       "3  national payment corporation india npci permit...  \n",
       "4  mark neilson 35yearold senior partner life ins...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f29609-67c1-44d5-bfda-35e06616606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = indian_news['clean_text']\n",
    "y = indian_news['category']\n",
    "\n",
    "x_text = dataset.text_stem.values\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(x_text)\n",
    "x_vectorized = vectorizer.transform(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7dbdd-a94a-4002-8a38-319015f381ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d379810-d498-48e1-a905-d059b4bf3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "           ############       Random Forest       ##############     \n",
    "  ######  Creates N number of trees using random starting values  ######\n",
    "########################################################################\n",
    "\n",
    "def random_forest_model(x, y):\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        min_samples_leaf=10,\n",
    "        n_estimators=200,\n",
    "        max_depth=5, \n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1,\n",
    "        max_features='auto'\n",
    "    )\n",
    "\n",
    "    rf_classifier.fit(x, y)\n",
    "\n",
    "    y_preds = rf_classifier.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "    ############       Gradient Boosting Classifier       ##############     \n",
    "######  Creates a random forest where each tree learns from the last  ######\n",
    "############################################################################\n",
    "\n",
    "def gradient_booster_model(x, y):\n",
    "    \n",
    "    gradient_booster = GradientBoostingClassifier(\n",
    "                            learning_rate=0.1,\n",
    "                            max_depth = 5,\n",
    "                            n_estimators=200)\n",
    "\n",
    "    gradient_booster.fit(x, y)\n",
    "    \n",
    "    y_preds = gradient_booster.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "#################################################################\n",
    "############         XG Boosting Classifier       ##############     \n",
    "    #######       Uses XG Boosting Algorthm       #######\n",
    "#################################################################\n",
    "\n",
    "def xgboost_model(x, y):\n",
    "    \n",
    "    xgboost = MOC(\n",
    "                XGBClassifier(\n",
    "                        base_score=None,\n",
    "                        booster=None,\n",
    "                        n_estimators=200,\n",
    "                        learning_rate=0.1,\n",
    "                        max_depth = 5\n",
    "                        ))\n",
    "\n",
    "    xgboost.fit(x, y)\n",
    "    \n",
    "    y_preds = xgboost.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "#################################################################\n",
    "#########         LightGMB Boosting Classifier       ###########     \n",
    "#######       Uses Light Gradient Boosting Algorthm       #######\n",
    "#################################################################\n",
    "\n",
    "def lgmboost_model(x, y):\n",
    "    \n",
    "    lgmboost = MOC(\n",
    "                LGBMClassifier(\n",
    "                learning_rate=0.1,\n",
    "                max_depth = 5,\n",
    "                n_estimators=200))\n",
    "\n",
    "    lgmboost.fit(x, y)\n",
    "    \n",
    "    y_preds = lgmboost.predict(x)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "\n",
    "####################################################################\n",
    "#########         Multinomial Naive Bayes Classifier     ###########     \n",
    "#######     Uses Naive Bayes as Classification Algorithm     #######\n",
    "####################################################################\n",
    "\n",
    "def nb_model(x, y):\n",
    "    \n",
    "    naive_bayes = MultinomialNB()\n",
    "    \n",
    "    naive_bayes.fit(x, y)\n",
    "    \n",
    "    y_preds = naive_bayes.predict(x)\n",
    "    \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf526a-6752-4316-ab83-9dc63806aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "    ############       Model Evaluation       ##############     \n",
    "   ######  Easily evaluate models for accuracy or any other metric  ######\n",
    "############################################################################\n",
    "\n",
    "def evaluate_classification_model(model, y_train, y_preds, df=False, full= False):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_train, y_preds).ravel()\n",
    "    ALL = TP + TN + FP + FN\n",
    "\n",
    "    accuracy = (TP + TN)/ALL\n",
    "    true_positive_rate = TP/(TP+FN)\n",
    "    false_positive_rate = FP/(FP+TN)\n",
    "    true_negative_rate = TN/(TN+FP)\n",
    "    false_negative_rate = FN/(FN+TP)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    \n",
    "    if df == False:\n",
    "        return accuracy\n",
    "\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    if full == True:\n",
    "        performance_df = pd.DataFrame(\n",
    "                                         {'model' : [model],\n",
    "                                          'accuracy' : [accuracy],\n",
    "                                          'f1_score' : [f1_score],\n",
    "                                          'precision' : [precision], \n",
    "                                          'recall' : [recall],\n",
    "                                          'true_positive_rate' : [true_positive_rate],\n",
    "                                          'false_positive_rate': [false_positive_rate], \n",
    "                                          'true_negative_rate' : [true_negative_rate], \n",
    "                                          'false_negative_rate': [false_negative_rate]\n",
    "                                          })\n",
    "        return performance_df\n",
    "\n",
    "    \n",
    "    \n",
    "    if full == False:\n",
    "        performance_df = pd.DataFrame(\n",
    "                                         {'model' : [model],\n",
    "                                          'accuracy' : [accuracy],\n",
    "                                          'f1_score' : [f1_score],\n",
    "                                          'precision' : [precision], \n",
    "                                          'recall' : [recall]\n",
    "                                         })\n",
    "\n",
    "    if df == True:\n",
    "        return performance_df\n",
    "\n",
    "\n",
    "def get_models(x_train, y_train): #, x_validate, y_validate):\n",
    "\n",
    "\n",
    "    rf_y_preds_train = random_forest_model(x_train, y_train)\n",
    "    #rf_y_preds_val = random_forest_model(x_validate, y_validate)\n",
    "\n",
    "    gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "    #gb_y_preds_val = gradient_booster_model(x_validate, y_validate)\n",
    "\n",
    "    xg_y_preds_train = xgboost_model(x_train, y_train)\n",
    "    #xg_y_preds_val = xgboost_booster_model(x_validate, y_validate)\n",
    "    \n",
    "    #lgm_y_preds_train = lgmboost_model(x_train, y_train)\n",
    "    #lgm_y_preds_val = lgmboost_booster_model(x_validate, y_validate)\n",
    "    \n",
    "    NB_y_preds_train = nb_model(x_train, y_train)\n",
    "    #nb_y_preds_val = nb_model(x_validate, y_validate)\n",
    "\n",
    "    performance_df = evaluate_classification_model('random_forest', y_train, rf_y_preds_train, df=True)\n",
    "    #performance_df = performance_df.append(evaluate_classification_model('rf_validate', y_validate, rf_y_preds_val, df=True))\n",
    "    performance_df = performance_df.append(evaluate_classification_model('gradient_booster', y_train, gb_y_preds_train, df=True))\n",
    "    #performance_df = performance_df.append(evaluate_classification_model('gb_validate', y_validate, gb_y_preds_val, df=True))\n",
    "    #performance_df = performance_df.append(evaluate_classification_model('xg_boost', y_train, xg_y_preds_train, df=True))\n",
    "    #performance_df = performance_df.append(evaluate_classification_model('xg_validate', y_validate, xg_y_preds_val, df=True))\n",
    "    #performance_df = performance_df.append(evaluate_classification_model('lgm_boost', y_train, lgm_y_preds_train, df=True))\n",
    "    #performance_df = performance_df.append(evaluate_classification_model('lgm_validate', y_validate, lgm_y_preds_val, df=True))\n",
    "    performance_df = performance_df.append(evaluate_classification_model('naive_bayes', y_train, NB_y_preds_train, df=True))\n",
    "    #performance_df = performance_df.append(evaluate_classification_model('NA_validate', y_validate, NB_y_preds_val, df=True))\n",
    "\n",
    "\n",
    "    \n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f6152-3620-4d85-bf26-f4d112b94025",
   "metadata": {},
   "source": [
    "### TODO \n",
    "- set up LGBMClassifier and XGBClassifier model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3aaa2c-2877-4909-869d-c6d724173f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = indian_news['clean_text']\n",
    "y = indian_news['category']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_vectorized, y, test_size = 0.3, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3bd599a-1937-4a51-8611-c92452d22348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65       world\n",
       "54       world\n",
       "27      sports\n",
       "58       world\n",
       "33      sports\n",
       "        ...   \n",
       "83    politics\n",
       "67       world\n",
       "25      sports\n",
       "68       world\n",
       "47      sports\n",
       "Name: category, Length: 70, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56d172ea-ccfb-4cf4-a1e7-7e510f4c367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "report = classification_report(y_train, gb_y_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bbbc2f0-b17b-4927-aaef-10daec60ed5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ccee10c-21c2-45ad-ac62-8254934c9419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       1.00      0.93      0.97        15\n",
      "    politics       1.00      1.00      1.00        17\n",
      "      sports       1.00      1.00      1.00        19\n",
      "       world       0.95      1.00      0.97        19\n",
      "\n",
      "    accuracy                           0.99        70\n",
      "   macro avg       0.99      0.98      0.98        70\n",
      "weighted avg       0.99      0.99      0.99        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e879dbc2-68b6-427e-89fe-52fa458db148",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hf/46l4fb0x1bj47kh2tdxlqp1h0000gn/T/ipykernel_58229/2803990512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#rf_y_preds_train = random_forest_model(x_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgb_y_preds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_booster_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mTN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb_y_preds_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mALL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "#rf_y_preds_train = random_forest_model(x_train, y_train)\n",
    "gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "TN, FP, FN, TP = confusion_matrix(y_train, gb_y_preds_train).ravel()\n",
    "ALL = TP + TN + FP + FN\n",
    "\n",
    "accuracy = (TP + TN)/ALL\n",
    "true_positive_rate = TP/(TP+FN)\n",
    "false_positive_rate = FP/(FP+TN)\n",
    "true_negative_rate = TN/(TN+FP)\n",
    "false_negative_rate = FN/(FN+TP)\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8baf5b-2381-413f-891a-e4b6ec9f3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_y_preds_train = gradient_booster_model(x_train, y_train)\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff2945-aee5-4334-b5ca-4d94817b9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "x_bow = cv.fit_transform(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92f576-9b61-4d13-bd53-d7d36a6cc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = indian_news['clean_text']\n",
    "y = indian_news['category']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "x_vectorized = cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_vectorized, y, test_size = 0.3, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d650f-c8a9-4f96-b422-e99769098ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "performance_df = get_models(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339b605-6711-44cb-85a2-ec644db1cbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fa894-540d-48d8-ba79-f77dfaf27f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1ca0f-cfac-45f6-9fb6-bda415ab1946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5e2d1-bddd-4f66-a67c-4eb118915db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
